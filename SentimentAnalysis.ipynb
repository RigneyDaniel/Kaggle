{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read and concatenate data into test and train sets.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "yelp = pd.read_csv('/Users/danielrigney/Documents/KAGGLE/SentimentAnalysis/input/yelp_labelled.txt', sep='\\t', names=['text', 'label'])\n",
    "imdb = pd.read_csv('/Users/danielrigney/Documents/KAGGLE/SentimentAnalysis/input/imdb_labelled.txt', sep='\\t', names=['text', 'label'])\n",
    "test = pd.read_csv('/Users/danielrigney/Documents/KAGGLE/SentimentAnalysis/input/amazon_cells_labelled.txt', sep='\\t', names=['text', 'label'])\n",
    "\n",
    "train = pd.concat([imdb,yelp], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the clean the data we execute the following tasks.\n",
    "- Ensure all text is lower case.\n",
    "- Remove any punctuation.\n",
    "- Remove common words that add no value (stop words).\n",
    "- Remove domain specific words.\n",
    "- Remove numerical values 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/danielrigney/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/stopwords.zip/stopwords/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/Users/danielrigney/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-4c461526fe37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mr'\\b'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstopword\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mr'\\b'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstopword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/danielrigney/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "## Ensure all letters are lowercase.\n",
    "train['text'] = train['text'].str.lower()\n",
    "\n",
    "## Remove punctuation.\n",
    "train['text'] = train['text'].str.replace(r'[^\\w\\s]', '')\n",
    "\n",
    "## Remove stop words.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = [r'\\b' + stopword + r'\\b' for stopword in stopwords.words('english')]\n",
    "stop_words = '|'.join(stop_words)\n",
    "train['text'] = train['text'].str.replace(stop_words, '')\n",
    "\n",
    "## Remove domain specific words.\n",
    "domain_specific_words = r'\\bactors\\b|\\bacting\\b|\\bcast\\b|\\bcharacters\\b|\\bfood\\b|\\bfilm\\b|\\bmovie\\b|\\brestaurant\\b'\n",
    "train['text'] = train['text'].str.replace(domain_specific_words, '')\n",
    "\n",
    "## Remove numerical values.\n",
    "numerical_values = r'\\b0\\b|\\b1\\b'\n",
    "train['text'] = train['text'].str.replace(numerical_values, '')\n",
    "\n",
    "## Print the ten most frequently used words.\n",
    "frequency = ' '.join(train['text']).split()\n",
    "frequency = pd.Series(frequency).value_counts()\n",
    "print(frequency[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must extract our features.\n",
    "We will use the frequency of each word in each review as our feature. This means our feature space is the size of all distinct words in the sample. (an extension of this method is to use the sequence of n consecutive words (n-gram) as features.)\n",
    "After counting the frequecy of each word, we normalise the frequency by scaling down the weight of words that occur often and scaling up the weight of words that occur less often. This is method is called term frequency - inverse term document frequency.\n",
    "Next we train the model using the features and the acutal sentiment scores. \n",
    "Finally, we use the trained model to predict the sentiment of the test reviews.\n",
    "Three models have been tested in this analysis. \n",
    "Naive Bayes - assumes each feature is independent, works well with categorical input.\n",
    "Logistic Regression - used to estimate the probability of a binary event. \n",
    "Support Vector Machine - more efficient than logisitic regression when the number of features is high (as is the case with this task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2661)\t0.187502656021\n",
      "  (0, 2890)\t0.224242606308\n",
      "  (0, 1538)\t0.129022749621\n",
      "  (0, 1928)\t0.226226815093\n",
      "  (0, 3164)\t0.262370939054\n",
      "  (0, 1160)\t0.153389126166\n",
      "  (0, 1776)\t0.227250814139\n",
      "  (0, 2935)\t0.130492025595\n",
      "  (0, 2177)\t0.291231562208\n",
      "  (0, 1541)\t0.129684987284\n",
      "  (0, 1477)\t0.321457750403\n",
      "  (0, 1376)\t0.21381256343\n",
      "  (0, 2883)\t0.08997459118\n",
      "  (0, 3065)\t0.262370939054\n",
      "  (0, 3042)\t0.324235155546\n",
      "  (0, 1253)\t0.223280672168\n",
      "  (0, 425)\t0.247603184942\n",
      "  (0, 651)\t0.376544613376\n",
      "  (1, 1263)\t0.339877460144\n",
      "  (1, 471)\t0.501576444364\n",
      "  (1, 1017)\t0.48181406076\n",
      "  (1, 3088)\t0.633055758523\n",
      "  (2, 1160)\t0.363736833495\n",
      "  (2, 2883)\t0.213359797456\n",
      "  (2, 1280)\t0.398142856633\n",
      "  :\t:\n",
      "  (1999, 2890)\t0.166322158229\n",
      "  (1999, 2935)\t0.0967867600452\n",
      "  (1999, 1541)\t0.0961881746297\n",
      "  (1999, 1477)\t0.119213622484\n",
      "  (1999, 2883)\t0.200204168653\n",
      "  (1999, 425)\t0.183648847032\n",
      "  (1999, 1456)\t0.160367130109\n",
      "  (1999, 2888)\t0.187369726766\n",
      "  (1999, 1962)\t0.111106041745\n",
      "  (1999, 3156)\t0.254806545614\n",
      "  (1999, 1892)\t0.117908936605\n",
      "  (1999, 2925)\t0.153725935039\n",
      "  (1999, 184)\t0.154749650022\n",
      "  (1999, 2015)\t0.167050342667\n",
      "  (1999, 1652)\t0.208127820781\n",
      "  (1999, 980)\t0.188702552642\n",
      "  (1999, 2894)\t0.153224994809\n",
      "  (1999, 517)\t0.230327571865\n",
      "  (1999, 2951)\t0.219081157332\n",
      "  (1999, 2475)\t0.254806545614\n",
      "  (1999, 375)\t0.240487263914\n",
      "  (1999, 1315)\t0.279285519363\n",
      "  (1999, 2206)\t0.279285519363\n",
      "  (1999, 3267)\t0.279285519363\n",
      "  (1999, 886)\t0.279285519363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import LinearSVC\\n\\n# Naive Bayes Model\\npipeline = Pipeline(steps=[('count_vectorizer', CountVectorizer()),\\n                           ('tfidf', TfidfTransformer()),\\n                           ('classifier', MultinomialNB())\\n                          ])\\npipeline.fit(train['text'], train['label'])\\npredictions_nb = pipeline.predict(test['text'])\\n\\n# Logistic Regression Model\\npipeline = Pipeline(steps=[('count_vectorizer', CountVectorizer()), \\n                          ('tfidf', TfidfTransformer()),\\n                          ('classifier', LogisticRegression(random_state=345))\\n                          ])\\npipeline.fit(train['text'], train['label'])\\npredictions_log = pipeline.predict(test['text'])\\n\\n# Support Vector Machine Model\\npipeline = Pipeline(steps=[('count_vectorizer', CountVectorizer()), \\n                          ('tfidf', TfidfTransformer()),\\n                          ('classifier', LinearSVC(random_state=345))\\n                          ])\\npipeline.fit(train['text'], train['label'])\\npredictions_svm = pipeline.predict(test['text'])\\n\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3 soln.\n",
    "# 3. Train the model.\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#count_vectorizer = CountVectorizer()\n",
    "#train_word_count = count_vectorizer.fit_transform(train['text'])\n",
    "\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "#train_tfidf = tfidf_transformer.fit_transform(train_word_count)\n",
    "\n",
    "## Naive Bayes Model\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#clf_nb = MultinomialNB().fit(train_tfidf, train['label'])\n",
    "#y_pred_nb = clf_nb.predict(tfidf_transformer.transform(count_vectorizer.transform(test['text'])))\n",
    "\n",
    "# Logistic Regression Model\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#clf_log = LogisticRegression(random_state=345).fit(train_tfidf, train['label'])\n",
    "#y_pred_log = clf_log.predict(tfidf_transformer.transform(count_vectorizer.transform(test['text'])))\n",
    "\n",
    "# Support Vector Machine\n",
    "#from sklearn.svm import LinearSVC\n",
    "\n",
    "#clf_svm = LinearSVC(random_state=345).fit(train_tfidf, train['label'])\n",
    "#y_pred_svm = clf_svm.predict(tfidf_transformer.transform(count_vectorizer.transform(test['text'])))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Naive Bayes Model\n",
    "pipeline = Pipeline(steps=[('count_vectorizer', CountVectorizer()),\n",
    "                           ('tfidf', TfidfTransformer()),\n",
    "                           ('classifier', MultinomialNB())\n",
    "                          ])\n",
    "pipeline.fit(train['text'], train['label'])\n",
    "predictions_nb = pipeline.predict(test['text'])\n",
    "\n",
    "# Logistic Regression Model\n",
    "pipeline = Pipeline(steps=[('count_vectorizer', CountVectorizer()), \n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('classifier', LogisticRegression(random_state=345))\n",
    "                          ])\n",
    "pipeline.fit(train['text'], train['label'])\n",
    "predictions_log = pipeline.predict(test['text'])\n",
    "\n",
    "# Support Vector Machine Model\n",
    "pipeline = Pipeline(steps=[('count_vectorizer', CountVectorizer()), \n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('classifier', LinearSVC(random_state=345))\n",
    "                          ])\n",
    "pipeline.fit(train['text'], train['label'])\n",
    "predictions_svm = pipeline.predict(test['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of the model we use three metrics.\n",
    "\n",
    "Recall = ratio of correctly identified positive reviews to the total number of actual positive reviews.\n",
    "\n",
    "Precision = ratio of correctly identified positive reviews to the total predicted positive reviews.\n",
    "\n",
    "Accuracy = ratio of correctly classified reviews divided by the total number of reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logistic_regression</th>\n",
       "      <th>naive_bayes</th>\n",
       "      <th>support_vector_machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.708556</td>\n",
       "      <td>0.740642</td>\n",
       "      <td>0.736631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.793706</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.807818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.588083</td>\n",
       "      <td>0.632124</td>\n",
       "      <td>0.642487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           logistic_regression  naive_bayes  support_vector_machine\n",
       "accuracy              0.708556     0.740642                0.736631\n",
       "precision             0.793706     0.824324                0.807818\n",
       "recall                0.588083     0.632124                0.642487"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluate the model.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "pd.DataFrame(data={'naive_bayes': [accuracy_score(test['label'], y_pred_nb),\n",
    "                                  precision_score(test['label'], y_pred_nb),\n",
    "                                  recall_score(test['label'], y_pred_nb)],\n",
    "                  'logistic_regression': [accuracy_score(test['label'], y_pred_log),\n",
    "                                         precision_score(test['label'], y_pred_log),\n",
    "                                         recall_score(test['label'], y_pred_log)],\n",
    "                  'support_vector_machine': [accuracy_score(test['label'], y_pred_svm),\n",
    "                                         precision_score(test['label'], y_pred_svm),\n",
    "                                         recall_score(test['label'], y_pred_svm)]},\n",
    "             index=['accuracy', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the accuracy of the naive bayes model performs better than the other two models with the sentiment of 74% of reviews correctly identified. The precision of the naive bayes model is quite high while the recall of the support vector machine is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
